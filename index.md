![](images/cover_4.jpeg)
*Blue Hours Seattle. 2022*

---


[Google Scholar](https://scholar.google.com/citations?user=liSP4cEAAAAJ&hl=en) / [Semantic Scholar](https://www.semanticscholar.org/author/Yao-Fu/46956602) / [Github](https://github.com/FranxYao) / [Twitter](https://twitter.com/Francis_YAO_) / [LinkedIn](https://www.linkedin.com/in/yao-fu-281847b5/) / [Instagram](https://www.instagram.com/franx_yao/) / [CV](cv.pdf) / [Blog](https://yaofu.notion.site/Yao-Fu-s-Blog-b536c3d6912149a395931f1e871370db)

Yao Fu угдт░Д. yao.fu@ed.ac.uk

I am a Ph.D. student at the University of Edinburgh (2020-) with professor [Mirella Lapata](https://homepages.inf.ed.ac.uk/mlap/) and currently a research intern at Allen Institute for AI.
I finished my M.S. at Columbia University (2018-2020) with professor [John Cunningham](https://stat.columbia.edu/~cunningham/) and my B.S. at Peking University (2013-2018) with professor [Yansong Feng](https://sites.google.com/site/ysfeng/home). 
Before Ph.D., I spent great time visiting professor [Alexander Rush](http://rush-nlp.com/) at Cornell Tech (2019-2020). 


I study large-scale probabilistic generative models for human language.

In the era of large language models, my research focuses on specialized language models, emergent abilities, complex reasoning, and new research paradigm in the large scale regime. My article on [tracing emergent abilities to their sources](https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1) is now an important roadmap about large language model evolution. 

Before the LLM era, I studied latent variable models for language generation and structure prediction. 

-----
### Selected Work
* [Blog Post 2022] [How does GPT Obtain its Ability? Tracing  Emergent Abilities of Language Models to their Sources](https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1)
  * __Yao Fu__, Hao Peng and Tushar Khot
  * Analysing sources of emergent abilities of Large Language Models from first principle.
  * [Hacker News](https://news.ycombinator.com/front?day=2022-12-15) top 3 trending.

* [ICLR 2023] _Complexity-Based Prompting for Multi-Step Reasoning_. [[paper](https://openreview.net/forum?id=yf1icZHC-l9)][[code](https://github.com/FranxYao/Complexity-Based-Prompting)]
  * __Yao Fu__, Hao Peng, Ashish Sabharwal, Peter Clark and Tushar Khot 
  * State-of-the-art reasoning performance on math word problems by prompting GPT3 with instances of complex reasoning chains.

* [ICML 2022] _Scaling Structured Inference with Randomization_. [[paper](https://arxiv.org/abs/2112.03638)][[code](https://github.com/FranxYao/RDP)]
  * __Yao Fu__, John P. Cunningham and Mirella Lapata
  *  A family of randomized dynamic programming algorithms for scaling up classical structured prediction algorithms of different inferences (partition, marginal, entropy, reparameterization) of structures (chains, trees, and general sum-product).

-----
### Preprints and Conference Publications

* [ICLR 2023] _Decomposed Prompting: A Modular Approach for Solving Complex Tasks_. [[paper](https://arxiv.org/abs/2210.02406)]
  * Tushar Khot, Harsh Trivedi, Matthew Finlayson, __Yao Fu__, Kyle Richardson, Peter Clark and Ashish Sabharwal
  * Decomposing complex task into simpler sub-tasks then solve each of them by prompting language models. 

* [Arxiv 2022] _Latent Topology Induction for Understanding Contextualized Representations_. [[paper](https://arxiv.org/abs/2206.01512)]
  * __Yao Fu__ and Mirella Lapata
  * Discovering hidden geometric structures of pretrained language models by unsupervised induction of a latent network. 

* [TACL 2022] _Data-to-text Generation with Variational Sequential Planning_.[[paper](https://arxiv.org/abs/2202.13756)][[code](https://github.com/ratishsp/data2text-seq-plan-py)]
  * Ratish Puduppully, __Yao Fu__, Mirella Lapata
  * A latent planning model for generating very long document.

* [NAACL 2021] _Noisy Labeled NER with Confidence Estimation_. [[paper](https://arxiv.org/abs/2104.04318)][[code](https://github.com/liukun95/Noisy-NER-Confidence-Estimation)]
  * Kun Liu\*, __Yao Fu__\*, Chuanqi Tan, Mosha Chen, Ningyu Zhang, Songfang Huang and Sheng Gao. \*Equal contribution.
  * A confidence estimation method for estimating label noise in NER annotations and a training method based on partial marginalization according to estimated noise.

* [ICLR 2021] _Probing BERT in Hyperbolic Spaces_. [[paper](https://openreview.net/forum?id=17VnwXYZyhH)][[code](https://github.com/FranxYao/PoincareProbe)]
  * Boli Chen\*, __Yao Fu__\*, Guangwei Xu, Pengjun Xie, Chuanqi Tan, Mosha Chen, Liping Jing. \*Equal contribution. 
  * A Poincare probe for recovering hierarchical structures from contextualized representations. Applied to probing syntax and sentiment in BERT. 

* [ICLR 2021] _Prototypical Representation Learning for Relation Extraction_. [[paper](https://openreview.net/forum?id=aCgLmfhIy_f)][[code](https://github.com/Alibaba-NLP/ProtoRE)]
  * Ning Ding, Xiaobin Wang, __Yao Fu__, Guangwei Xu, Rui Wang, Pengjun Xie, Ying Shen, Fei Huang, Hai-Tao Zheng, Rui Zhang
  * A representation learning method for embedding relation prototypes on hyperspheres. Applied to supervised, semi-supervised, and few-shot relational learning. 

* [AAAI 2021] _Nested Named Entity Recognition with Partially Observed TreeCRFs_. [[paper](https://arxiv.org/abs/2012.08478)][[code](https://github.com/FranxYao/Partially-Observed-TreeCRFs)]
   *  __Yao Fu__\*, Chuanqi Tan\*, Mosha Chen, Songfang Huang, Fei Huang. \*Equal contribution. 
   * A Masked Inside algorithm for efficient partial marginalization of TreeCRFs. Applied to Nested NER.

* [NeurIPS 2020] _Latent Template Induction with Gumbel-CRFs_. [[paper](https://arxiv.org/abs/2011.14244)][[code](https://github.com/FranxYao/Gumbel-CRF)]
   * __Yao Fu__, Chuanqi Tan, Mosha Chen, Bin Bi, Yansong Feng and Alexander Rush. 
   * A Gumbel-FFBS algorithm for reparameterizing and relaxing CRFs. Applied to controllable text generation with latent templates.

* [NeurIPS 2019] _Paraphrase Generation with Latent Bag of Words_. [[paper](https://arxiv.org/abs/2001.01941)][[code](https://github.com/FranxYao/dgm_latent_bow)]
   * **Yao Fu**, Yansong Feng and John Cunningham. 
   * A differentiable planning and realization model with latent bag of words by Gumbel-topK reparameterization. Applied to paraphrase generation.

* [INLG 2019] _Rethinking Text Attribute Transfer: A Lexical Analysis_. [[paper](https://arxiv.org/abs/1909.12335)][[code](https://github.com/FranxYao/pivot_analysis)]
   * **Yao Fu**, Hao Zhou, Jiaze Chen and Lei Li. 
   * A series of text mining algorithms for discovering words with strong influence on classification. Applied to analysing text attribute transfer models. 

* [NAACL 2018] _Natural Answer Generation with Heterogeneous Memory_. [[paper](https://www.aclweb.org/anthology/N18-1017/)]
   * **Yao Fu** and Yansong Feng. 
   * An attention mechanism fusing information from different source of knowledge. Applied to answer sentence generation.

-----
### Workshop Publications
* [EMNLP FigLang 2022] _Just DREAM about it: Figurative Language Understanding with DREAM-FLUTE._ [[paper](https://arxiv.org/abs/2210.16407)][[code](https://github.com/allenai/dream)]
  * The Third Workshop on Figurative Language Processing. In conjunction with EMNLP 2022
  * Yuling Gu, **Yao Fu**, Valentina Pyatkin, Ian Magnusson, Bhavana Dalvi Mishra and Peter Clark
  * **Ranked top 1** in the task leaderboard. A mental model utilizing scene elaboration for understanding figurative language.


-----

### Blog and Open Source

* [A Closer Look at Large Language Models Emergent Abilities](https://yaofu.notion.site/A-Closer-Look-at-Large-Language-Models-Emergent-Abilities-493876b55df5479d80686f68a1abd72f).
  * Yao Fu, Hao Peng and Tushar Khot
  * Scrutinizing Large Language Model Emergent Abilities to see if the paradigm is really shifting to in-context learning.

* [Why S4 is Good at Long Sequence: Remembering a Sequence with Online Function Approximation](https://yaofu.notion.site/Why-S4-is-Good-at-Long-Sequence-Remembering-a-Sequence-with-Online-Function-Approximation-836fc54a49aa413b84997a265132f13f). Feb 2022. 
  * Yao Fu
  * explaining the S4 model with function approximation theory.

* [Deep Generative Models for Natural Language Processing](https://github.com/franxyao/Deep-Generative-Models-for-Natural-Language-Processing). 2019
  * Yao Fu
  * A roadmap tracking past, present, and future about generative models for NLP.

* [Distributional Generalization in Natural Language Processing](https://github.com/FranxYao/Distributional-Generalization-in-Natural-Language-Processing). 2020
  * Yao Fu
  * A roadmap tracking past, present, and future about generalization challenges in NLP.

-----
### Teaching 

* Peking University. Empirical Methods for Natural Language Processing. 2022 Spring. 
  * Guest lecture on Text Generation. Tought by Yansong Feng.
* University of Edinburgh. Natural Language Understanding. 2022 Spring. 
  * Teaching Assistant. Tought by Alexandra Birch, Frank Keller, and Laura Perez.
* University of Edinburgh. [Probabilistic Modeling and Reasoning](http://www.inf.ed.ac.uk/teaching/courses/pmr/21-22/). 2022 Spring. 
  * Teaching Assistant. Tought by Michael Gutmann.
* Peking University. Empirical Methods for Natural Language Processing. 2021 Spring. 
  * Guest lecture on Text Generation. Tought by Yansong Feng.
* Alibaba DAMO Academy. Advanced Probabilistic Machine Learning Seminar. 2020 Spring. 
  * Instructor. 
* Columbia University. [COMS 4995 Applied Machine Learning](http://www.cs.columbia.edu/~amueller/comsw4995s19/), 2019 Spring.
  * Course Assistant. Tought by Andreas Muller. 

-----

### Internships
* Jul 22 - . Allen Institute for Artificial Intelligence. Research Intern. Seattle. 
  * with Dr. [Tushar Khot](https://scholar.google.com/citations?user=_8mkIjgAAAAJ&hl=en)
* Jan 20 - Oct 20. Alibaba Damo Academy. Research Intern. Beijing and Hangzhou
  * with Dr. [Chuanqi Tan](https://scholar.google.com/citations?user=tOfo4ncAAAAJ&hl=zh-CN) and [Mosha Chen](https://scholar.google.com/citations?user=6bTGGDAAAAAJ&hl=en)
* May 19 - Aug 19. Tencent AI Lab. Research Intern. Seattle
  * with Dr. [Kun Xu](https://sites.google.com/view/kunxu/home) and Dr. [Dong Yu](https://sites.google.com/view/dongyu888/)
* Dec 17 - Aug 18. Bytedance AI Lab. Research Intern. Beijing
  * with Prof. [Hao Zhou](https://scholar.google.com/citations?user=q3WaozcAAAAJ&hl=en) and Prof. [Lei Li](https://scholar.google.com/citations?user=BYXqAlwAAAAJ&hl=en)





