![](images/cover.jpg)

---


[Google Scholar](https://scholar.google.com/citations?user=liSP4cEAAAAJ&hl=en) | [Semantic Scholar](https://www.semanticscholar.org/author/Yao-Fu/46956602) | [Github](https://github.com/FranxYao) | [LinkedIn](https://www.linkedin.com/in/yao-fu-281847b5/) | [Instagram](https://www.instagram.com/franx_yao/) | [Galary](galary.md)

My name is Yao Fu угдт░Д. My email address is yao.fu@ed.ac.uk

I am a Ph.D. student at the University of Edinburgh (2020-) with Prof. [Mirella Lapata](https://homepages.inf.ed.ac.uk/mlap/). 
I finished my M.S. at Columbia University (2018-2020) with Prof. [John Cunningham](https://stat.columbia.edu/~cunningham/) doing deep generative models and my B.S. at Peking University (2013-2018) with Prof. [Yansong Feng](https://sites.google.com/site/ysfeng/home) doing text generation. 
Before Ph.D., I spent great time visiting Prof. [Alexander Rush](http://rush-nlp.com/) at Cornell University (2019-2020) doing structured latent variable models. 


My research focus is structured prediction and text generation, 
and I believe that to persue the most fundamental principles of human language, one must utilize a spectrum of interdisciplinary techniques: ML, NLP, Linguistics, Statistics, Optimization, Geometry, .etc. 
Many of my papers lay in the intersection of these areas. 
In general, I derive probabilistic models guided by Bayesian principles, equipped with modern neural architectures, utilizing efficient inference, and grounded to linguistics and real-world scenarios. 

In terms of specific topics, I am interested in 
* NLP: Semantics; Text Generation; Linguistic Structure Prediction
* ML: Generalization; Discrete Latent Structures; Deep Generative Models

Many topics that I'm interested in are covered by the following reading list:
* Deep Generative Models for Natural Language Processing. ([github](https://github.com/franxyao/Deep-Generative-Models-for-Natural-Language-Processing))
* Compositional Generalization in Natural Language Processing. ([github](https://github.com/FranxYao/Compositional-Generalization-in-Natural-Language-Processing))

-----

### Publications
* [NAACL 2021] _Noisy Labeled NER with Confidence Estimation_. [[paper](https://arxiv.org/abs/2104.04318)][[code](https://github.com/liukun95/Noisy-NER-Confidence-Estimation)]
  * Kun Liu\*, __Yao Fu__\*, Chuanqi Tan, Mosha Chen, Ningyu Zhang, Songfang Huang and Sheng Gao. \*Equal contribution.
  * A confidence estimation method for estimating label noise in NER annotations and a training method based on partial marginalization according to estimated noise.
* [ICLR 2021] _Probing BERT in Hyperbolic Spaces_. [[paper](https://openreview.net/forum?id=17VnwXYZyhH)][[code](https://github.com/FranxYao/PoincareProbe)]
  * Boli Chen\*, __Yao Fu__\*, Guangwei Xu, Pengjun Xie, Chuanqi Tan, Mosha Chen, Liping Jing. \*Equal contribution. 
  * A Poincare probe for recovering hierarchical structures from contextualized representations. Applied to probing syntax and sentiment in BERT. 
* [ICLR 2021] _Prototypical Representation Learning for Relation Extraction_. [[paper](https://openreview.net/forum?id=aCgLmfhIy_f)][[code](https://github.com/Alibaba-NLP/ProtoRE)]
  * Ning Ding, Xiaobin Wang, __Yao Fu__, Guangwei Xu, Rui Wang, Pengjun Xie, Ying Shen, Fei Huang, Hai-Tao Zheng, Rui Zhang
  * A representation learning method for embedding relation prototypes on hyperspheres. Applied to supervised, semi-supervised, and few-shot relational learning. 
* [AAAI 2021] _Nested Named Entity Recognition with Partially Observed TreeCRFs_. [[paper](https://arxiv.org/abs/2012.08478)][[code](https://github.com/FranxYao/Partially-Observed-TreeCRFs)]
   *  __Yao Fu__\*, Chuanqi Tan\*, Mosha Chen, Songfang Huang, Fei Huang. \*Equal contribution. 
   * A Masked Inside algorithm for efficient partial marginalization of TreeCRFs. Applied to Nested NER.
* [NeurIPS 2020] _Latent Template Induction with Gumbel-CRFs_. [[paper](https://arxiv.org/abs/2011.14244)][[code](https://github.com/FranxYao/Gumbel-CRF)]
   * __Yao Fu__, Chuanqi Tan, Mosha Chen, Bin Bi, Yansong Feng and Alexander Rush. 
   * A Gumbel-FFBS algorithm for reparameterizing and relaxing CRFs. Applied to controllable text generation with latent templates.
* [NeurIPS 2019] _Paraphrase Generation with Latent Bag of Words_. [[paper](https://arxiv.org/abs/2001.01941)][[code](https://github.com/FranxYao/dgm_latent_bow)]
   * **Yao Fu**, Yansong Feng and John Cunningham. 
   * A differentiable planning and realization model with latent bag of words by Gumbel-topK reparameterization. Applied to paraphrase generation.
* [INLG 2019] _Rethinking Text Attribute Transfer: A Lexical Analysis_. [[paper](https://arxiv.org/abs/1909.12335)][[code](https://github.com/FranxYao/pivot_analysis)]
   * **Yao Fu**, Hao Zhou, Jiaze Chen and Lei Li. 
   * A series of text mining algorithms for discovering words with strong influence on classification. Applied to analysing text attribute transfer models. 
* [NAACL 2018] _Natural Answer Generation with Heterogeneous Memory_. [[paper](https://www.aclweb.org/anthology/N18-1017/)]
   * **Yao Fu** and Yansong Feng. 
   * An attention mechanism fusing information from different source of knowledge. Applied to answer sentence generation.

-----

### Teaching 

* PKU EECS. Empirical Methods for Natural Language Processing. 2021 Spring. Guest lecture on Text Generation. Tought by Prof. Yansong Feng.
* Alibaba Advanced Probabilistic Machine Learning Seminar. 2020 Spring. Instructor. 
* [Columbia COMS 4995 Applied Machine Learning](http://www.cs.columbia.edu/~amueller/comsw4995s19/), 19 Spring, Course Assistant. Tought by Prof. Andreas Muller. 


-----

### Resources and Tutorials 

* Deep Structured Prediction: Inference, Reparameterization and Applications. Talk at Bytedance. Jun 2021 [[pdf](https://github.com/FranxYao/franxyao.github.io/blob/master/blog/bytedance%20structured%20prediction%20pub.pdf)]
* How to write Variational Inference and Generative Models for NLP: a recipe. Talk at Edinburgh. Mar 2021 [[pdf](https://github.com/FranxYao/Deep-Generative-Models-for-Natural-Language-Processing/blob/master/src/VI4NLP_Recipe.pdf)]

-----

### Internships
* Jan 20 - Oct 20. Alibaba Damo Academy. Natural Language Processing Research Intern. Beijing and Hangzhou
* May 19 - Aug 19. Tencent AI Lab. Natural Language Processing Research Intern. Seattle
* Dec 17 - Aug 18. Bytedance AI Lab. Natural Language Processing Research Intern. Beijing





